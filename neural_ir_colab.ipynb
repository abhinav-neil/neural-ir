{"cells":[{"cell_type":"markdown","metadata":{"id":"g9UZ5cfxGY8Q"},"source":["#1. Setting up the enviroments "]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":44181,"status":"ok","timestamp":1677968218137,"user":{"displayName":"Abhinav Bhuyan","userId":"03657681414650155581"},"user_tz":-60},"id":"LUmuKS4RRLYd","outputId":"605103e0-d01c-4020-a041-664f9a9d2305"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/Colab Notebooks/IR1_A1_part2\n"]}],"source":["# mount google drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","# clone repo\n","#!git clone https://{PERSONAL_ACCESS_TOKEN}@github.com/{USERNAME}/{REPO-NAME}.git\n","# change working directory to repo\n","%cd /content/drive/MyDrive/Colab\\ Notebooks/IR1_A1_part2/ \n","# install\n","!pip install -r requirements.txt  > /dev/null"]},{"cell_type":"markdown","metadata":{"id":"bioHuGfoF1tZ"},"source":["#2. Train and evaluate"]},{"cell_type":"markdown","metadata":{"id":"hNflD7z_H8HM"},"source":["## 2.1 Training "]},{"cell_type":"code","execution_count":2,"metadata":{"id":"6An5g74MF7tj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677975171765,"user_tz":-60,"elapsed":6606059,"user":{"displayName":"Abhinav Bhuyan","userId":"03657681414650155581"}},"outputId":"eadd42a2-92c8-4714-8cc7-15a6dcf7f69a"},"outputs":[{"output_type":"stream","name":"stdout","text":["2023-03-04 22:22:46.619311: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-03-04 22:22:46.760048: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2023-03-04 22:22:47.496457: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2023-03-04 22:22:47.496549: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2023-03-04 22:22:47.496568: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","Downloading (…)okenizer_config.json: 100% 28.0/28.0 [00:00<00:00, 4.14kB/s]\n","Downloading (…)lve/main/config.json: 100% 483/483 [00:00<00:00, 77.3kB/s]\n","Downloading (…)solve/main/vocab.txt: 100% 232k/232k [00:00<00:00, 2.54MB/s]\n","Downloading (…)/main/tokenizer.json: 100% 466k/466k [00:00<00:00, 4.16MB/s]\n","reading triplets from train_triplets.tsv: 100% 96922/96922 [00:00<00:00, 350793.91it/s]\n","Downloading (…)\"pytorch_model.bin\";: 100% 268M/268M [00:02<00:00, 99.0MB/s]\n","max_steps is given, it will override any value given in num_train_epochs\n","Using cuda_amp half precision backend\n","/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 96922\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 4000\n","  Number of trainable parameters = 66985530\n","  2% 100/4000 [00:26<15:11,  4.28it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:15<00:00, 22.91it/s]\n","{'eval_RR@10': 0.06899801587301586, 'eval_R@1000': 0.9825, 'eval_nDCG@10': 0.08962408468359706, 'epoch': 0.07}\n","  2% 100/4000 [02:42<15:11,  4.28it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-100\n","Configuration saved in output/sparse/model/checkpoint-100/config.json\n","Model weights saved in output/sparse/model/checkpoint-100/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-100/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-100/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-200] due to args.save_total_limit\n","  5% 200/4000 [03:07<14:31,  4.36it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:16<00:00, 22.80it/s]\n","{'eval_RR@10': 0.09237499999999997, 'eval_R@1000': 0.9825, 'eval_nDCG@10': 0.10917511672069971, 'epoch': 0.13}\n","  5% 200/4000 [05:24<14:31,  4.36it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-200\n","Configuration saved in output/sparse/model/checkpoint-200/config.json\n","Model weights saved in output/sparse/model/checkpoint-200/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-200/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-200/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-4000] due to args.save_total_limit\n","  8% 300/4000 [05:49<14:02,  4.39it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:15<00:00, 22.90it/s]\n","{'eval_RR@10': 0.048894841269841255, 'eval_R@1000': 0.9825, 'eval_nDCG@10': 0.05615325519690007, 'epoch': 0.2}\n","  8% 300/4000 [08:05<14:02,  4.39it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-300\n","Configuration saved in output/sparse/model/checkpoint-300/config.json\n","Model weights saved in output/sparse/model/checkpoint-300/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-300/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-300/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-100] due to args.save_total_limit\n"," 10% 400/4000 [08:31<13:21,  4.49it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:15<00:00, 22.87it/s]\n","{'eval_RR@10': 0.026125, 'eval_R@1000': 0.9825, 'eval_nDCG@10': 0.032541111455281845, 'epoch': 0.26}\n"," 10% 400/4000 [10:47<13:21,  4.49it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-400\n","Configuration saved in output/sparse/model/checkpoint-400/config.json\n","Model weights saved in output/sparse/model/checkpoint-400/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-400/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-400/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-300] due to args.save_total_limit\n","{'loss': 129458.832, 'learning_rate': 4.75e-06, 'epoch': 0.33}\n"," 12% 500/4000 [11:12<13:19,  4.38it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:15<00:00, 22.90it/s]\n","{'eval_RR@10': 0.00914484126984127, 'eval_R@1000': 0.9825, 'eval_nDCG@10': 0.015877567254705665, 'epoch': 0.33}\n"," 12% 500/4000 [13:28<13:19,  4.38it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-500\n","Configuration saved in output/sparse/model/checkpoint-500/config.json\n","Model weights saved in output/sparse/model/checkpoint-500/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-500/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-500/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-400] due to args.save_total_limit\n"," 15% 600/4000 [13:53<12:32,  4.52it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:15<00:00, 22.89it/s]\n","{'eval_RR@10': 0.008416666666666666, 'eval_R@1000': 0.9825, 'eval_nDCG@10': 0.011906546032578968, 'epoch': 0.4}\n"," 15% 600/4000 [16:09<12:32,  4.52it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-600\n","Configuration saved in output/sparse/model/checkpoint-600/config.json\n","Model weights saved in output/sparse/model/checkpoint-600/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-600/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-600/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-500] due to args.save_total_limit\n"," 18% 700/4000 [16:35<12:16,  4.48it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:15<00:00, 22.90it/s]\n","{'eval_RR@10': 0.00125, 'eval_R@1000': 0.9825, 'eval_nDCG@10': 0.0031546487678572877, 'epoch': 0.46}\n"," 18% 700/4000 [18:51<12:16,  4.48it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-700\n","Configuration saved in output/sparse/model/checkpoint-700/config.json\n","Model weights saved in output/sparse/model/checkpoint-700/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-700/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-700/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-600] due to args.save_total_limit\n"," 20% 800/4000 [19:16<12:34,  4.24it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:16<00:00, 22.83it/s]\n","{'eval_RR@10': 0.00125, 'eval_R@1000': 0.9825, 'eval_nDCG@10': 0.003598706921956405, 'epoch': 0.53}\n"," 20% 800/4000 [21:32<12:34,  4.24it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-800\n","Configuration saved in output/sparse/model/checkpoint-800/config.json\n","Model weights saved in output/sparse/model/checkpoint-800/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-800/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-800/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-700] due to args.save_total_limit\n"," 22% 900/4000 [21:58<11:43,  4.41it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:15<00:00, 22.86it/s]\n","{'eval_RR@10': 0.0014583333333333332, 'eval_R@1000': 0.9825, 'eval_nDCG@10': 0.003243991050595311, 'epoch': 0.59}\n"," 22% 900/4000 [24:14<11:43,  4.41it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-900\n","Configuration saved in output/sparse/model/checkpoint-900/config.json\n","Model weights saved in output/sparse/model/checkpoint-900/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-900/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-900/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-800] due to args.save_total_limit\n","{'loss': 298.5599, 'learning_rate': 9.750000000000002e-06, 'epoch': 0.66}\n"," 25% 1000/4000 [24:39<12:05,  4.13it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:15<00:00, 22.88it/s]\n","{'eval_RR@10': 0.0, 'eval_R@1000': 0.9825, 'eval_nDCG@10': 0.006985343392063292, 'epoch': 0.66}\n"," 25% 1000/4000 [26:56<12:05,  4.13it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-1000\n","Configuration saved in output/sparse/model/checkpoint-1000/config.json\n","Model weights saved in output/sparse/model/checkpoint-1000/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-1000/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-1000/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-900] due to args.save_total_limit\n"," 28% 1100/4000 [27:21<11:30,  4.20it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:15<00:00, 22.87it/s]\n","{'eval_RR@10': 0.0, 'eval_R@1000': 0.9825, 'eval_nDCG@10': 0.0030824743622485497, 'epoch': 0.73}\n"," 28% 1100/4000 [29:37<11:30,  4.20it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-1100\n","Configuration saved in output/sparse/model/checkpoint-1100/config.json\n","Model weights saved in output/sparse/model/checkpoint-1100/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-1100/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-1100/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-1000] due to args.save_total_limit\n"," 30% 1200/4000 [30:03<10:56,  4.27it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:15<00:00, 22.86it/s]\n","{'eval_RR@10': 0.0008333333333333333, 'eval_R@1000': 0.9825, 'eval_nDCG@10': 0.005308031558224253, 'epoch': 0.79}\n"," 30% 1200/4000 [32:19<10:56,  4.27it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-1200\n","Configuration saved in output/sparse/model/checkpoint-1200/config.json\n","Model weights saved in output/sparse/model/checkpoint-1200/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-1200/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-1200/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-1100] due to args.save_total_limit\n"," 32% 1300/4000 [32:45<10:09,  4.43it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:15<00:00, 22.88it/s]\n","{'eval_RR@10': 0.0, 'eval_R@1000': 0.9825, 'eval_nDCG@10': 0.0015773243839286438, 'epoch': 0.86}\n"," 32% 1300/4000 [35:01<10:09,  4.43it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-1300\n","Configuration saved in output/sparse/model/checkpoint-1300/config.json\n","Model weights saved in output/sparse/model/checkpoint-1300/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-1300/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-1300/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-1200] due to args.save_total_limit\n"," 35% 1400/4000 [35:26<10:11,  4.25it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:16<00:00, 22.77it/s]\n","{'eval_RR@10': 0.0007142857142857143, 'eval_R@1000': 0.9825, 'eval_nDCG@10': 0.010178255086768017, 'epoch': 0.92}\n"," 35% 1400/4000 [37:43<10:11,  4.25it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-1400\n","Configuration saved in output/sparse/model/checkpoint-1400/config.json\n","Model weights saved in output/sparse/model/checkpoint-1400/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-1400/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-1400/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-1300] due to args.save_total_limit\n","{'loss': 266.5154, 'learning_rate': 1.475e-05, 'epoch': 0.99}\n"," 38% 1500/4000 [38:08<09:01,  4.61it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:15<00:00, 22.85it/s]\n","{'eval_RR@10': 0.000625, 'eval_R@1000': 0.9825, 'eval_nDCG@10': 0.003934418725907076, 'epoch': 0.99}\n"," 38% 1500/4000 [40:24<09:01,  4.61it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-1500\n","Configuration saved in output/sparse/model/checkpoint-1500/config.json\n","Model weights saved in output/sparse/model/checkpoint-1500/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-1500/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-1500/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-1400] due to args.save_total_limit\n"," 40% 1600/4000 [40:49<09:12,  4.34it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:16<00:00, 22.84it/s]\n","{'eval_RR@10': 0.0, 'eval_R@1000': 0.9825, 'eval_nDCG@10': 0.0014453241315894394, 'epoch': 1.06}\n"," 40% 1600/4000 [43:05<09:12,  4.34it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-1600\n","Configuration saved in output/sparse/model/checkpoint-1600/config.json\n","Model weights saved in output/sparse/model/checkpoint-1600/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-1600/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-1600/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-1500] due to args.save_total_limit\n"," 42% 1700/4000 [43:31<08:55,  4.30it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:15<00:00, 22.88it/s]\n","{'eval_RR@10': 0.0, 'eval_R@1000': 0.9825, 'eval_nDCG@10': 0.0034394140144926146, 'epoch': 1.12}\n"," 42% 1700/4000 [45:47<08:55,  4.30it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-1700\n","Configuration saved in output/sparse/model/checkpoint-1700/config.json\n","Model weights saved in output/sparse/model/checkpoint-1700/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-1700/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-1700/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-1600] due to args.save_total_limit\n"," 45% 1800/4000 [46:12<08:58,  4.08it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:15<00:00, 22.89it/s]\n","{'eval_RR@10': 0.0, 'eval_R@1000': 0.9825, 'eval_nDCG@10': 0.008838350865385656, 'epoch': 1.19}\n"," 45% 1800/4000 [48:28<08:58,  4.08it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-1800\n","Configuration saved in output/sparse/model/checkpoint-1800/config.json\n","Model weights saved in output/sparse/model/checkpoint-1800/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-1800/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-1800/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-1700] due to args.save_total_limit\n"," 48% 1900/4000 [48:54<07:54,  4.42it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:14<00:00, 23.03it/s]\n","{'eval_RR@10': 0.0030555555555555557, 'eval_R@1000': 0.9825, 'eval_nDCG@10': 0.012249141028915217, 'epoch': 1.25}\n"," 48% 1900/4000 [51:09<07:54,  4.42it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-1900\n","Configuration saved in output/sparse/model/checkpoint-1900/config.json\n","Model weights saved in output/sparse/model/checkpoint-1900/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-1900/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-1900/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-1800] due to args.save_total_limit\n","{'loss': 266.1919, 'learning_rate': 1.9750000000000002e-05, 'epoch': 1.32}\n"," 50% 2000/4000 [51:35<07:51,  4.24it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:14<00:00, 23.03it/s]\n","{'eval_RR@10': 0.0, 'eval_R@1000': 0.9825, 'eval_nDCG@10': 0.00549633590725293, 'epoch': 1.32}\n"," 50% 2000/4000 [53:50<07:51,  4.24it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-2000\n","Configuration saved in output/sparse/model/checkpoint-2000/config.json\n","Model weights saved in output/sparse/model/checkpoint-2000/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-2000/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-2000/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-1900] due to args.save_total_limit\n"," 52% 2100/4000 [54:15<07:26,  4.26it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:14<00:00, 23.02it/s]\n","{'eval_RR@10': 0.0, 'eval_R@1000': 0.9825, 'eval_nDCG@10': 0.006215299971712818, 'epoch': 1.39}\n"," 52% 2100/4000 [56:30<07:26,  4.26it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-2100\n","Configuration saved in output/sparse/model/checkpoint-2100/config.json\n","Model weights saved in output/sparse/model/checkpoint-2100/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-2100/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-2100/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-2000] due to args.save_total_limit\n"," 55% 2200/4000 [56:56<07:17,  4.11it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:15<00:00, 23.00it/s]\n","{'eval_RR@10': 0.0012142857142857142, 'eval_R@1000': 0.9825, 'eval_nDCG@10': 0.00532549059118286, 'epoch': 1.45}\n"," 55% 2200/4000 [59:11<07:17,  4.11it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-2200\n","Configuration saved in output/sparse/model/checkpoint-2200/config.json\n","Model weights saved in output/sparse/model/checkpoint-2200/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-2200/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-2200/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-2100] due to args.save_total_limit\n"," 57% 2300/4000 [59:37<06:25,  4.41it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:15<00:00, 22.99it/s]\n","{'eval_RR@10': 0.000625, 'eval_R@1000': 0.9825, 'eval_nDCG@10': 0.002153382790366965, 'epoch': 1.52}\n"," 57% 2300/4000 [1:01:52<06:25,  4.41it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-2300\n","Configuration saved in output/sparse/model/checkpoint-2300/config.json\n","Model weights saved in output/sparse/model/checkpoint-2300/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-2300/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-2300/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-2200] due to args.save_total_limit\n"," 60% 2400/4000 [1:02:17<05:47,  4.60it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:14<00:00, 23.06it/s]\n","{'eval_RR@10': 0.000625, 'eval_R@1000': 0.9825, 'eval_nDCG@10': 0.004653382790366966, 'epoch': 1.58}\n"," 60% 2400/4000 [1:04:32<05:47,  4.60it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-2400\n","Configuration saved in output/sparse/model/checkpoint-2400/config.json\n","Model weights saved in output/sparse/model/checkpoint-2400/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-2400/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-2400/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-2300] due to args.save_total_limit\n","{'loss': 266.7021, 'learning_rate': 2.4750000000000002e-05, 'epoch': 1.65}\n"," 62% 2500/4000 [1:04:58<05:41,  4.40it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:14<00:00, 23.08it/s]\n","{'eval_RR@10': 0.0, 'eval_R@1000': 0.9825, 'eval_nDCG@10': 0.0050889128040299955, 'epoch': 1.65}\n"," 62% 2500/4000 [1:07:13<05:41,  4.40it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-2500\n","Configuration saved in output/sparse/model/checkpoint-2500/config.json\n","Model weights saved in output/sparse/model/checkpoint-2500/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-2500/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-2500/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-2400] due to args.save_total_limit\n"," 65% 2600/4000 [1:07:38<05:29,  4.24it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:14<00:00, 23.08it/s]\n","{'eval_RR@10': 0.00125, 'eval_R@1000': 0.9825, 'eval_nDCG@10': 0.004653382790366966, 'epoch': 1.72}\n"," 65% 2600/4000 [1:09:53<05:29,  4.24it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-2600\n","Configuration saved in output/sparse/model/checkpoint-2600/config.json\n","Model weights saved in output/sparse/model/checkpoint-2600/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-2600/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-2600/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-2500] due to args.save_total_limit\n"," 68% 2700/4000 [1:10:19<04:41,  4.61it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:14<00:00, 23.08it/s]\n","{'eval_RR@10': 0.0, 'eval_R@1000': 0.9825, 'eval_nDCG@10': 0.0015773243839286438, 'epoch': 1.78}\n"," 68% 2700/4000 [1:12:34<04:41,  4.61it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-2700\n","Configuration saved in output/sparse/model/checkpoint-2700/config.json\n","Model weights saved in output/sparse/model/checkpoint-2700/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-2700/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-2700/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-2600] due to args.save_total_limit\n"," 70% 2800/4000 [1:13:00<04:45,  4.20it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:14<00:00, 23.06it/s]\n","{'eval_RR@10': 0.0, 'eval_R@1000': 0.9825, 'eval_nDCG@10': 0.0, 'epoch': 1.85}\n"," 70% 2800/4000 [1:15:15<04:45,  4.20it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-2800\n","Configuration saved in output/sparse/model/checkpoint-2800/config.json\n","Model weights saved in output/sparse/model/checkpoint-2800/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-2800/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-2800/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-2700] due to args.save_total_limit\n"," 72% 2900/4000 [1:15:40<04:18,  4.26it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:15<00:00, 22.98it/s]\n","{'eval_RR@10': 0.0007142857142857143, 'eval_R@1000': 0.9825, 'eval_nDCG@10': 0.002153382790366965, 'epoch': 1.91}\n"," 72% 2900/4000 [1:17:55<04:18,  4.26it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-2900\n","Configuration saved in output/sparse/model/checkpoint-2900/config.json\n","Model weights saved in output/sparse/model/checkpoint-2900/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-2900/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-2900/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-2800] due to args.save_total_limit\n","{'loss': 266.8365, 'learning_rate': 2.975e-05, 'epoch': 1.98}\n"," 75% 3000/4000 [1:18:21<04:01,  4.14it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:15<00:00, 22.98it/s]\n","{'eval_RR@10': 0.0, 'eval_R@1000': 0.9825, 'eval_nDCG@10': 0.0014453241315894394, 'epoch': 1.98}\n"," 75% 3000/4000 [1:20:36<04:01,  4.14it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-3000\n","Configuration saved in output/sparse/model/checkpoint-3000/config.json\n","Model weights saved in output/sparse/model/checkpoint-3000/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-3000/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-3000/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-2900] due to args.save_total_limit\n"," 78% 3100/4000 [1:21:01<03:27,  4.33it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:14<00:00, 23.04it/s]\n","{'eval_RR@10': 0.0, 'eval_R@1000': 0.9825, 'eval_nDCG@10': 0.0031718166449865724, 'epoch': 2.05}\n"," 78% 3100/4000 [1:23:16<03:27,  4.33it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-3100\n","Configuration saved in output/sparse/model/checkpoint-3100/config.json\n","Model weights saved in output/sparse/model/checkpoint-3100/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-3100/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-3100/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-3000] due to args.save_total_limit\n"," 80% 3200/4000 [1:23:42<03:14,  4.11it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:14<00:00, 23.08it/s]\n","{'eval_RR@10': 0.0, 'eval_R@1000': 0.9825, 'eval_nDCG@10': 0.0015051499783199059, 'epoch': 2.11}\n"," 80% 3200/4000 [1:25:57<03:14,  4.11it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-3200\n","Configuration saved in output/sparse/model/checkpoint-3200/config.json\n","Model weights saved in output/sparse/model/checkpoint-3200/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-3200/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-3200/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-3100] due to args.save_total_limit\n"," 82% 3300/4000 [1:26:23<02:37,  4.44it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:14<00:00, 23.06it/s]\n","{'eval_RR@10': 0.0, 'eval_R@1000': 0.9825, 'eval_nDCG@10': 0.005, 'epoch': 2.18}\n"," 82% 3300/4000 [1:28:38<02:37,  4.44it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-3300\n","Configuration saved in output/sparse/model/checkpoint-3300/config.json\n","Model weights saved in output/sparse/model/checkpoint-3300/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-3300/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-3300/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-3200] due to args.save_total_limit\n"," 85% 3400/4000 [1:29:03<02:17,  4.37it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:14<00:00, 23.08it/s]\n","{'eval_RR@10': 0.0, 'eval_R@1000': 0.9825, 'eval_nDCG@10': 0.003243991050595311, 'epoch': 2.24}\n"," 85% 3400/4000 [1:31:18<02:17,  4.37it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-3400\n","Configuration saved in output/sparse/model/checkpoint-3400/config.json\n","Model weights saved in output/sparse/model/checkpoint-3400/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-3400/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-3400/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-3300] due to args.save_total_limit\n","{'loss': 266.5353, 'learning_rate': 3.475e-05, 'epoch': 2.31}\n"," 88% 3500/4000 [1:31:43<01:58,  4.23it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:14<00:00, 23.08it/s]\n","{'eval_RR@10': 0.0005555555555555556, 'eval_R@1000': 0.9825, 'eval_nDCG@10': 0.006534236935619435, 'epoch': 2.31}\n"," 88% 3500/4000 [1:33:58<01:58,  4.23it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-3500\n","Configuration saved in output/sparse/model/checkpoint-3500/config.json\n","Model weights saved in output/sparse/model/checkpoint-3500/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-3500/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-3500/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-3400] due to args.save_total_limit\n"," 90% 3600/4000 [1:34:23<01:33,  4.30it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:14<00:00, 23.07it/s]\n","{'eval_RR@10': 0.001, 'eval_R@1000': 0.9825, 'eval_nDCG@10': 0.008375098717890012, 'epoch': 2.38}\n"," 90% 3600/4000 [1:36:38<01:33,  4.30it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-3600\n","Configuration saved in output/sparse/model/checkpoint-3600/config.json\n","Model weights saved in output/sparse/model/checkpoint-3600/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-3600/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-3600/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-3500] due to args.save_total_limit\n"," 92% 3700/4000 [1:37:03<01:10,  4.23it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:15<00:00, 22.98it/s]\n","{'eval_RR@10': 0.0, 'eval_R@1000': 0.9825, 'eval_nDCG@10': 0.003358360319468755, 'epoch': 2.44}\n"," 92% 3700/4000 [1:39:19<01:10,  4.23it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-3700\n","Configuration saved in output/sparse/model/checkpoint-3700/config.json\n","Model weights saved in output/sparse/model/checkpoint-3700/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-3700/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-3700/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-3600] due to args.save_total_limit\n"," 95% 3800/4000 [1:39:44<00:48,  4.16it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:14<00:00, 23.09it/s]\n","{'eval_RR@10': 0.0, 'eval_R@1000': 0.9825, 'eval_nDCG@10': 0.0047319731517859315, 'epoch': 2.51}\n"," 95% 3800/4000 [1:41:59<00:48,  4.16it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-3800\n","Configuration saved in output/sparse/model/checkpoint-3800/config.json\n","Model weights saved in output/sparse/model/checkpoint-3800/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-3800/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-3800/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-3700] due to args.save_total_limit\n"," 98% 3900/4000 [1:42:24<00:23,  4.31it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:14<00:00, 23.08it/s]\n","{'eval_RR@10': 0.0015000000000000002, 'eval_R@1000': 0.9825, 'eval_nDCG@10': 0.009342230735976367, 'epoch': 2.57}\n"," 98% 3900/4000 [1:44:39<00:23,  4.31it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-3900\n","Configuration saved in output/sparse/model/checkpoint-3900/config.json\n","Model weights saved in output/sparse/model/checkpoint-3900/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-3900/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-3900/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-3800] due to args.save_total_limit\n","{'loss': 266.96, 'learning_rate': 3.9750000000000004e-05, 'epoch': 2.64}\n","100% 4000/4000 [1:45:05<00:00,  4.28it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:14<00:00, 23.09it/s]\n","{'eval_RR@10': 0.0007142857142857143, 'eval_R@1000': 0.9825, 'eval_nDCG@10': 0.003934418725907076, 'epoch': 2.64}\n","100% 4000/4000 [1:47:20<00:00,  4.28it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-4000\n","Configuration saved in output/sparse/model/checkpoint-4000/config.json\n","Model weights saved in output/sparse/model/checkpoint-4000/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-4000/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-4000/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-3900] due to args.save_total_limit\n","INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:14<00:00, 23.07it/s]\n","{'eval_RR@10': 0.0007142857142857143, 'eval_R@1000': 0.9825, 'eval_nDCG@10': 0.003934418725907076, 'epoch': 2.64}\n","100% 4000/4000 [1:49:37<00:00,  4.28it/s]\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","INFO:neural_ir.trainer.hf_trainer:Loading best model from output/sparse/model/checkpoint-200 (score: 0.09237499999999997).\n","{'train_runtime': 6578.3209, 'train_samples_per_second': 38.916, 'train_steps_per_second': 0.608, 'train_loss': 16419.641640625, 'epoch': 2.64}\n","100% 4000/4000 [1:49:38<00:00,  1.64s/it]\n","INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model\n","Configuration saved in output/sparse/model/config.json\n","Model weights saved in output/sparse/model/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/special_tokens_map.json\n"]}],"source":["!python -m neural_ir.train --train_batch_size 64 --eval_batch_size 64 sparse"]},{"cell_type":"markdown","metadata":{"id":"XeinjLMiIIzg"},"source":["## 2.2 Prediction "]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1xUWlbSSH6ha","executionInfo":{"status":"ok","timestamp":1677976597993,"user_tz":-60,"elapsed":1425791,"user":{"displayName":"Abhinav Bhuyan","userId":"03657681414650155581"}},"outputId":"e699033b-6f60-46dc-f7bb-88ec6ad5ff16"},"outputs":[{"output_type":"stream","name":"stdout","text":["2023-03-05 00:12:53.277280: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-03-05 00:12:53.440421: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2023-03-05 00:12:54.203608: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2023-03-05 00:12:54.203715: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2023-03-05 00:12:54.203733: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","reading pairs from collection.tsv: 100% 96270/96270 [00:00<00:00, 794794.18it/s]\n","reading pairs from test_queries.tsv: 100% 500/500 [00:00<00:00, 1307451.37it/s]\n","Encoding documents: 100% 6017/6017 [01:58<00:00, 50.88it/s]\n","Encoding queries and search: 100% 32/32 [00:01<00:00, 24.92it/s]\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\n","2023-03-05 00:15:09,852 INFO  [main] index.IndexCollection (IndexCollection.java:391) - Setting log level to INFO\n","2023-03-05 00:15:09,855 INFO  [main] index.IndexCollection (IndexCollection.java:394) - Starting indexer...\n","2023-03-05 00:15:09,855 INFO  [main] index.IndexCollection (IndexCollection.java:395) - ============ Loading Parameters ============\n","2023-03-05 00:15:09,855 INFO  [main] index.IndexCollection (IndexCollection.java:396) - DocumentCollection path: output/sparse/docs\n","2023-03-05 00:15:09,856 INFO  [main] index.IndexCollection (IndexCollection.java:397) - CollectionClass: JsonVectorCollection\n","2023-03-05 00:15:09,856 INFO  [main] index.IndexCollection (IndexCollection.java:398) - Generator: DefaultLuceneDocumentGenerator\n","2023-03-05 00:15:09,856 INFO  [main] index.IndexCollection (IndexCollection.java:399) - Threads: 12\n","2023-03-05 00:15:09,857 INFO  [main] index.IndexCollection (IndexCollection.java:400) - Language: en\n","2023-03-05 00:15:09,857 INFO  [main] index.IndexCollection (IndexCollection.java:401) - Stemmer: porter\n","2023-03-05 00:15:09,857 INFO  [main] index.IndexCollection (IndexCollection.java:402) - Keep stopwords? false\n","2023-03-05 00:15:09,857 INFO  [main] index.IndexCollection (IndexCollection.java:403) - Stopwords: null\n","2023-03-05 00:15:09,858 INFO  [main] index.IndexCollection (IndexCollection.java:404) - Store positions? false\n","2023-03-05 00:15:09,858 INFO  [main] index.IndexCollection (IndexCollection.java:405) - Store docvectors? false\n","2023-03-05 00:15:09,858 INFO  [main] index.IndexCollection (IndexCollection.java:406) - Store document \"contents\" field? false\n","2023-03-05 00:15:09,858 INFO  [main] index.IndexCollection (IndexCollection.java:407) - Store document \"raw\" field? false\n","2023-03-05 00:15:09,859 INFO  [main] index.IndexCollection (IndexCollection.java:408) - Additional fields to index: []\n","2023-03-05 00:15:09,859 INFO  [main] index.IndexCollection (IndexCollection.java:409) - Optimize (merge segments)? false\n","2023-03-05 00:15:09,859 INFO  [main] index.IndexCollection (IndexCollection.java:410) - Whitelist: null\n","2023-03-05 00:15:09,860 INFO  [main] index.IndexCollection (IndexCollection.java:411) - Pretokenized?: true\n","2023-03-05 00:15:09,860 INFO  [main] index.IndexCollection (IndexCollection.java:412) - Index path: output/sparse/index\n","2023-03-05 00:15:09,863 INFO  [main] index.IndexCollection (IndexCollection.java:450) - ============ Indexing Collection ============\n","2023-03-05 00:15:10,592 INFO  [main] index.IndexCollection (IndexCollection.java:565) - Thread pool with 12 threads initialized.\n","2023-03-05 00:15:10,593 INFO  [main] index.IndexCollection (IndexCollection.java:567) - Initializing collection in output/sparse/docs\n","2023-03-05 00:15:10,638 INFO  [main] index.IndexCollection (IndexCollection.java:576) - 1 file found\n","2023-03-05 00:15:10,638 INFO  [main] index.IndexCollection (IndexCollection.java:577) - Starting to index...\n","2023-03-05 00:16:10,642 INFO  [main] index.IndexCollection (IndexCollection.java:589) - 0 documents indexed\n","2023-03-05 00:17:10,644 INFO  [main] index.IndexCollection (IndexCollection.java:589) - 10,000 documents indexed\n","2023-03-05 00:18:10,645 INFO  [main] index.IndexCollection (IndexCollection.java:589) - 10,000 documents indexed\n","2023-03-05 00:19:10,647 INFO  [main] index.IndexCollection (IndexCollection.java:589) - 20,000 documents indexed\n","2023-03-05 00:20:10,649 INFO  [main] index.IndexCollection (IndexCollection.java:589) - 20,000 documents indexed\n","2023-03-05 00:21:10,650 INFO  [main] index.IndexCollection (IndexCollection.java:589) - 30,000 documents indexed\n","2023-03-05 00:22:10,652 INFO  [main] index.IndexCollection (IndexCollection.java:589) - 30,000 documents indexed\n","2023-03-05 00:23:10,653 INFO  [main] index.IndexCollection (IndexCollection.java:589) - 40,000 documents indexed\n","2023-03-05 00:24:10,655 INFO  [main] index.IndexCollection (IndexCollection.java:589) - 40,000 documents indexed\n","2023-03-05 00:25:10,657 INFO  [main] index.IndexCollection (IndexCollection.java:589) - 50,000 documents indexed\n","2023-03-05 00:26:10,659 INFO  [main] index.IndexCollection (IndexCollection.java:589) - 60,000 documents indexed\n","2023-03-05 00:27:10,661 INFO  [main] index.IndexCollection (IndexCollection.java:589) - 60,000 documents indexed\n","2023-03-05 00:28:10,663 INFO  [main] index.IndexCollection (IndexCollection.java:589) - 70,000 documents indexed\n","2023-03-05 00:29:10,664 INFO  [main] index.IndexCollection (IndexCollection.java:589) - 70,000 documents indexed\n","2023-03-05 00:30:10,666 INFO  [main] index.IndexCollection (IndexCollection.java:589) - 80,000 documents indexed\n","2023-03-05 00:31:10,668 INFO  [main] index.IndexCollection (IndexCollection.java:589) - 80,000 documents indexed\n","2023-03-05 00:32:10,670 INFO  [main] index.IndexCollection (IndexCollection.java:589) - 90,000 documents indexed\n","2023-03-05 00:32:37,644 DEBUG [pool-2-thread-1] index.IndexCollection$LocalIndexerThread (IndexCollection.java:356) - docs/docs.jsonl: 96270 docs added.\n","2023-03-05 00:32:41,784 INFO  [main] index.IndexCollection (IndexCollection.java:633) - Indexing Complete! 96,270 documents indexed\n","2023-03-05 00:32:41,785 INFO  [main] index.IndexCollection (IndexCollection.java:634) - ============ Final Counter Values ============\n","2023-03-05 00:32:41,785 INFO  [main] index.IndexCollection (IndexCollection.java:635) - indexed:           96,270\n","2023-03-05 00:32:41,785 INFO  [main] index.IndexCollection (IndexCollection.java:636) - unindexable:            0\n","2023-03-05 00:32:41,785 INFO  [main] index.IndexCollection (IndexCollection.java:637) - empty:                  0\n","2023-03-05 00:32:41,785 INFO  [main] index.IndexCollection (IndexCollection.java:638) - skipped:                0\n","2023-03-05 00:32:41,785 INFO  [main] index.IndexCollection (IndexCollection.java:639) - errors:                 0\n","2023-03-05 00:32:41,790 INFO  [main] index.IndexCollection (IndexCollection.java:642) - Total 96,270 documents indexed in 00:17:31\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","2023-03-05 00:32:45.420983: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2023-03-05 00:32:45.421087: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2023-03-05 00:32:45.421107: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","Running output/sparse/queries/test.tsv topics, saving to output/sparse/test_run.trec...\n","100% 500/500 [03:45<00:00,  2.22it/s]\n"]}],"source":["!python -m neural_ir.rank_sparse\n","# !python -m neural_ir.evaluate sparse # eval"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}