{"cells":[{"cell_type":"markdown","metadata":{"id":"g9UZ5cfxGY8Q"},"source":["#1. Setting up the enviroments "]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5568,"status":"ok","timestamp":1678016402867,"user":{"displayName":"Abhinav Bhuyan","userId":"03657681414650155581"},"user_tz":-60},"id":"LUmuKS4RRLYd","outputId":"c540ae5a-c2a4-4f5e-f75f-3dd34be0be3b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/Colab Notebooks/IR1_A1_part2\n"]}],"source":["# mount google drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","# clone repo\n","#!git clone https://{PERSONAL_ACCESS_TOKEN}@github.com/{USERNAME}/{REPO-NAME}.git\n","# change working directory to repo\n","%cd /content/drive/MyDrive/Colab\\ Notebooks/IR1_A1_part2/ \n","# install\n","!pip install -r requirements.txt  > /dev/null"]},{"cell_type":"markdown","source":["# 2. Run tests"],"metadata":{"id":"T6rEylpjFGj-"}},{"cell_type":"code","source":["!pytest -v neural_ir/public_tests/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ikrghbfgFFAb","executionInfo":{"status":"ok","timestamp":1678025870430,"user_tz":-60,"elapsed":16942,"user":{"displayName":"Abhinav Bhuyan","userId":"03657681414650155581"}},"outputId":"dd948e72-1719-4b52-9be1-c04656cbd12f"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m============================= test session starts ==============================\u001b[0m\n","platform linux -- Python 3.8.10, pytest-3.6.4, py-1.11.0, pluggy-0.7.1 -- /usr/bin/python3\n","cachedir: .pytest_cache\n","rootdir: /content/drive/MyDrive/Colab Notebooks/IR1_A1_part2, inifile:\n","plugins: typeguard-2.7.1\n","collected 16 items                                                             \u001b[0m\n","\n","neural_ir/public_tests/test_cross_encoder.py::test_score_pairs \u001b[32mPASSED\u001b[0m\u001b[36m    [  6%]\u001b[0m\n","neural_ir/public_tests/test_cross_encoder.py::test_forward \u001b[32mPASSED\u001b[0m\u001b[36m        [ 12%]\u001b[0m\n","neural_ir/public_tests/test_datasets.py::test_pair_dataset \u001b[32mPASSED\u001b[0m\u001b[36m        [ 18%]\u001b[0m\n","neural_ir/public_tests/test_datasets.py::test_triplet_dataset \u001b[32mPASSED\u001b[0m\u001b[36m     [ 25%]\u001b[0m\n","neural_ir/public_tests/test_dense_encoder.py::test_encode_text \u001b[32mPASSED\u001b[0m\u001b[36m    [ 31%]\u001b[0m\n","neural_ir/public_tests/test_dense_encoder.py::test_score_pairs \u001b[32mPASSED\u001b[0m\u001b[36m    [ 37%]\u001b[0m\n","neural_ir/public_tests/test_dense_encoder.py::test_forward \u001b[32mPASSED\u001b[0m\u001b[36m        [ 43%]\u001b[0m\n","neural_ir/public_tests/test_read_files.py::test_read_pairs \u001b[32mPASSED\u001b[0m\u001b[36m        [ 50%]\u001b[0m\n","neural_ir/public_tests/test_read_files.py::test_read_triplets \u001b[32mPASSED\u001b[0m\u001b[36m     [ 56%]\u001b[0m\n","neural_ir/public_tests/test_run_files.py::test_ce_run_file_available \u001b[32mPASSED\u001b[0m\u001b[36m [ 62%]\u001b[0m\n","neural_ir/public_tests/test_run_files.py::test_dense_run_file_available \u001b[32mPASSED\u001b[0m\u001b[36m [ 68%]\u001b[0m\n","neural_ir/public_tests/test_run_files.py::test_sparse_run_file_available \u001b[32mPASSED\u001b[0m\u001b[36m [ 75%]\u001b[0m\n","neural_ir/public_tests/test_sparse_encoder.py::test_encode_text \u001b[32mPASSED\u001b[0m\u001b[36m   [ 81%]\u001b[0m\n","neural_ir/public_tests/test_sparse_encoder.py::test_score_pairs \u001b[32mPASSED\u001b[0m\u001b[36m   [ 87%]\u001b[0m\n","neural_ir/public_tests/test_sparse_encoder.py::test_forward \u001b[32mPASSED\u001b[0m\u001b[36m       [ 93%]\u001b[0m\n","neural_ir/public_tests/test_sparse_regularizer.py::test_regularizer \u001b[32mPASSED\u001b[0m\u001b[36m [100%]\u001b[0m\n","\n","\u001b[32m\u001b[1m========================== 16 passed in 15.49 seconds ==========================\u001b[0m\n"]}]},{"cell_type":"markdown","metadata":{"id":"bioHuGfoF1tZ"},"source":["#2. Train and evaluate"]},{"cell_type":"markdown","metadata":{"id":"hNflD7z_H8HM"},"source":["## 2.1 Training "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6An5g74MF7tj","colab":{"base_uri":"https://localhost:8080/"},"outputId":"701c91b2-4330-403c-da4d-65fc0c82c31c"},"outputs":[{"output_type":"stream","name":"stdout","text":["2023-03-05 17:16:57.053147: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-03-05 17:16:57.193099: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2023-03-05 17:16:57.947862: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2023-03-05 17:16:57.947957: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2023-03-05 17:16:57.947977: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","reading triplets from train_triplets.tsv: 100% 96922/96922 [00:00<00:00, 419765.62it/s]\n","reading pairs from collection.tsv: 100% 96270/96270 [00:00<00:00, 782741.18it/s]\n","reading pairs from dev_queries.tsv: 100% 200/200 [00:00<00:00, 1398101.33it/s]\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight']\n","- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","max_steps is given, it will override any value given in num_train_epochs\n","Using cuda_amp half precision backend\n","/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 96922\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 4000\n","  Number of trainable parameters = 66362880\n"," 10% 400/4000 [01:03<09:14,  6.50it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [01:34<00:00, 32.75it/s]\n","{'eval_nDCG@10': 0.5965738762915763, 'eval_R@1000': 0.9825, 'eval_RR@10': 0.5556567460317461, 'epoch': 0.26}\n"," 10% 400/4000 [02:39<09:14,  6.50it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/your_creativity/model/checkpoint-400\n","Configuration saved in output/your_creativity/model/checkpoint-400/config.json\n","Model weights saved in output/your_creativity/model/checkpoint-400/pytorch_model.bin\n","tokenizer config file saved in output/your_creativity/model/checkpoint-400/tokenizer_config.json\n","Special tokens file saved in output/your_creativity/model/checkpoint-400/special_tokens_map.json\n","Deleting older checkpoint [output/your_creativity/model/checkpoint-500] due to args.save_total_limit\n","{'loss': 305.7905, 'learning_rate': 4.96e-06, 'epoch': 0.33}\n"," 20% 800/4000 [03:44<08:39,  6.16it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [01:35<00:00, 32.65it/s]\n","{'eval_nDCG@10': 0.4995527794893351, 'eval_R@1000': 0.9825, 'eval_RR@10': 0.47296825396825404, 'epoch': 0.53}\n"," 20% 800/4000 [05:19<08:39,  6.16it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/your_creativity/model/checkpoint-800\n","Configuration saved in output/your_creativity/model/checkpoint-800/config.json\n","Model weights saved in output/your_creativity/model/checkpoint-800/pytorch_model.bin\n","tokenizer config file saved in output/your_creativity/model/checkpoint-800/tokenizer_config.json\n","Special tokens file saved in output/your_creativity/model/checkpoint-800/special_tokens_map.json\n","Deleting older checkpoint [output/your_creativity/model/checkpoint-4000] due to args.save_total_limit\n","{'loss': 295.2875, 'learning_rate': 9.940000000000001e-06, 'epoch': 0.66}\n"," 30% 1200/4000 [06:25<07:24,  6.30it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model:  94% 2905/3106 [01:29<00:06, 32.87it/s]"]}],"source":["!python -m neural_ir.train --train_batch_size 64 --eval_batch_size 64 --eval_steps 400 your_creativity"]},{"cell_type":"markdown","metadata":{"id":"XeinjLMiIIzg"},"source":["## 2.2 Prediction "]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1xUWlbSSH6ha","executionInfo":{"status":"ok","timestamp":1678036569805,"user_tz":-60,"elapsed":47084,"user":{"displayName":"Abhinav Bhuyan","userId":"03657681414650155581"}},"outputId":"2af8eb1f-ce92-460a-b0b8-ad6c22dd280d"},"outputs":[{"output_type":"stream","name":"stdout","text":["2023-03-05 17:15:24.141408: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-03-05 17:15:24.321574: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2023-03-05 17:15:25.212127: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2023-03-05 17:15:25.212222: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2023-03-05 17:15:25.212240: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","reading pairs from collection.tsv: 100% 96270/96270 [00:00<00:00, 635710.71it/s]\n","reading pairs from test_queries.tsv: 100% 500/500 [00:00<00:00, 1516378.89it/s]\n","Reading pairs from data/test_bm25.trec: 492626it [00:00, 602679.10it/s]\n","Writing run file to output/ce/test_run.trec: 100% 500/500 [00:00<00:00, 6774.78it/s]\n"]}],"source":["!python -m neural_ir.rerank"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}