{"cells":[{"cell_type":"markdown","metadata":{"id":"g9UZ5cfxGY8Q"},"source":["#1. Setting up the enviroments "]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5568,"status":"ok","timestamp":1678016402867,"user":{"displayName":"Abhinav Bhuyan","userId":"03657681414650155581"},"user_tz":-60},"id":"LUmuKS4RRLYd","outputId":"c540ae5a-c2a4-4f5e-f75f-3dd34be0be3b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/Colab Notebooks/IR1_A1_part2\n"]}],"source":["# mount google drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","# clone repo\n","#!git clone https://{PERSONAL_ACCESS_TOKEN}@github.com/{USERNAME}/{REPO-NAME}.git\n","# change working directory to repo\n","%cd /content/drive/MyDrive/Colab\\ Notebooks/IR1_A1_part2/ \n","# install\n","!pip install -r requirements.txt  > /dev/null"]},{"cell_type":"markdown","source":["# 2. Run tests"],"metadata":{"id":"T6rEylpjFGj-"}},{"cell_type":"code","source":["!pytest -v neural_ir/public_tests/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ikrghbfgFFAb","executionInfo":{"status":"ok","timestamp":1678025870430,"user_tz":-60,"elapsed":16942,"user":{"displayName":"Abhinav Bhuyan","userId":"03657681414650155581"}},"outputId":"dd948e72-1719-4b52-9be1-c04656cbd12f"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m============================= test session starts ==============================\u001b[0m\n","platform linux -- Python 3.8.10, pytest-3.6.4, py-1.11.0, pluggy-0.7.1 -- /usr/bin/python3\n","cachedir: .pytest_cache\n","rootdir: /content/drive/MyDrive/Colab Notebooks/IR1_A1_part2, inifile:\n","plugins: typeguard-2.7.1\n","collected 16 items                                                             \u001b[0m\n","\n","neural_ir/public_tests/test_cross_encoder.py::test_score_pairs \u001b[32mPASSED\u001b[0m\u001b[36m    [  6%]\u001b[0m\n","neural_ir/public_tests/test_cross_encoder.py::test_forward \u001b[32mPASSED\u001b[0m\u001b[36m        [ 12%]\u001b[0m\n","neural_ir/public_tests/test_datasets.py::test_pair_dataset \u001b[32mPASSED\u001b[0m\u001b[36m        [ 18%]\u001b[0m\n","neural_ir/public_tests/test_datasets.py::test_triplet_dataset \u001b[32mPASSED\u001b[0m\u001b[36m     [ 25%]\u001b[0m\n","neural_ir/public_tests/test_dense_encoder.py::test_encode_text \u001b[32mPASSED\u001b[0m\u001b[36m    [ 31%]\u001b[0m\n","neural_ir/public_tests/test_dense_encoder.py::test_score_pairs \u001b[32mPASSED\u001b[0m\u001b[36m    [ 37%]\u001b[0m\n","neural_ir/public_tests/test_dense_encoder.py::test_forward \u001b[32mPASSED\u001b[0m\u001b[36m        [ 43%]\u001b[0m\n","neural_ir/public_tests/test_read_files.py::test_read_pairs \u001b[32mPASSED\u001b[0m\u001b[36m        [ 50%]\u001b[0m\n","neural_ir/public_tests/test_read_files.py::test_read_triplets \u001b[32mPASSED\u001b[0m\u001b[36m     [ 56%]\u001b[0m\n","neural_ir/public_tests/test_run_files.py::test_ce_run_file_available \u001b[32mPASSED\u001b[0m\u001b[36m [ 62%]\u001b[0m\n","neural_ir/public_tests/test_run_files.py::test_dense_run_file_available \u001b[32mPASSED\u001b[0m\u001b[36m [ 68%]\u001b[0m\n","neural_ir/public_tests/test_run_files.py::test_sparse_run_file_available \u001b[32mPASSED\u001b[0m\u001b[36m [ 75%]\u001b[0m\n","neural_ir/public_tests/test_sparse_encoder.py::test_encode_text \u001b[32mPASSED\u001b[0m\u001b[36m   [ 81%]\u001b[0m\n","neural_ir/public_tests/test_sparse_encoder.py::test_score_pairs \u001b[32mPASSED\u001b[0m\u001b[36m   [ 87%]\u001b[0m\n","neural_ir/public_tests/test_sparse_encoder.py::test_forward \u001b[32mPASSED\u001b[0m\u001b[36m       [ 93%]\u001b[0m\n","neural_ir/public_tests/test_sparse_regularizer.py::test_regularizer \u001b[32mPASSED\u001b[0m\u001b[36m [100%]\u001b[0m\n","\n","\u001b[32m\u001b[1m========================== 16 passed in 15.49 seconds ==========================\u001b[0m\n"]}]},{"cell_type":"markdown","metadata":{"id":"bioHuGfoF1tZ"},"source":["#2. Train and evaluate"]},{"cell_type":"markdown","metadata":{"id":"hNflD7z_H8HM"},"source":["## 2.1 Training "]},{"cell_type":"code","execution_count":10,"metadata":{"id":"6An5g74MF7tj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678032693304,"user_tz":-60,"elapsed":6643249,"user":{"displayName":"Abhinav Bhuyan","userId":"03657681414650155581"}},"outputId":"5a207fdc-3554-4d8e-8561-4cd1452f17d8"},"outputs":[{"output_type":"stream","name":"stdout","text":["2023-03-05 14:20:52.479025: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-03-05 14:20:52.616872: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2023-03-05 14:20:53.377029: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2023-03-05 14:20:53.377126: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2023-03-05 14:20:53.377146: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","reading triplets from train_triplets.tsv: 100% 96922/96922 [00:00<00:00, 423988.22it/s]\n","reading pairs from collection.tsv: 100% 96270/96270 [00:00<00:00, 785931.46it/s]\n","reading pairs from dev_queries.tsv: 100% 200/200 [00:00<00:00, 1278751.22it/s]\n","max_steps is given, it will override any value given in num_train_epochs\n","Using cuda_amp half precision backend\n","/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 96922\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 4000\n","  Number of trainable parameters = 66985530\n","  2% 100/4000 [00:24<15:22,  4.23it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:17<00:00, 22.55it/s]\n","{'eval_RR@10': 0.08330753968253965, 'eval_nDCG@10': 0.10682467526319309, 'eval_R@1000': 0.9825, 'epoch': 0.07}\n","  2% 100/4000 [02:42<15:22,  4.23it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-100\n","Configuration saved in output/sparse/model/checkpoint-100/config.json\n","Model weights saved in output/sparse/model/checkpoint-100/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-100/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-100/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-200] due to args.save_total_limit\n","  5% 200/4000 [03:09<14:37,  4.33it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:16<00:00, 22.77it/s]\n","{'eval_RR@10': 0.13405753968253967, 'eval_nDCG@10': 0.1592160202126843, 'eval_R@1000': 0.9825, 'epoch': 0.13}\n","  5% 200/4000 [05:25<14:37,  4.33it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-200\n","Configuration saved in output/sparse/model/checkpoint-200/config.json\n","Model weights saved in output/sparse/model/checkpoint-200/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-200/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-200/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-4000] due to args.save_total_limit\n","  8% 300/4000 [05:51<14:09,  4.35it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:16<00:00, 22.77it/s]\n","{'eval_RR@10': 0.1457876984126984, 'eval_nDCG@10': 0.18431663715089264, 'eval_R@1000': 0.9825, 'epoch': 0.2}\n","  8% 300/4000 [08:08<14:09,  4.35it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-300\n","Configuration saved in output/sparse/model/checkpoint-300/config.json\n","Model weights saved in output/sparse/model/checkpoint-300/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-300/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-300/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-100] due to args.save_total_limit\n"," 10% 400/4000 [08:34<13:37,  4.41it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:16<00:00, 22.78it/s]\n","{'eval_RR@10': 0.4157698412698413, 'eval_nDCG@10': 0.4458992588862272, 'eval_R@1000': 0.9825, 'epoch': 0.26}\n"," 10% 400/4000 [10:50<13:37,  4.41it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-400\n","Configuration saved in output/sparse/model/checkpoint-400/config.json\n","Model weights saved in output/sparse/model/checkpoint-400/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-400/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-400/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-200] due to args.save_total_limit\n","{'loss': 42.078, 'learning_rate': 4.87e-06, 'epoch': 0.33}\n"," 12% 500/4000 [11:16<13:31,  4.31it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:16<00:00, 22.75it/s]\n","{'eval_RR@10': 0.5246289682539682, 'eval_nDCG@10': 0.5572863415812188, 'eval_R@1000': 0.9825, 'epoch': 0.33}\n"," 12% 500/4000 [13:33<13:31,  4.31it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-500\n","Configuration saved in output/sparse/model/checkpoint-500/config.json\n","Model weights saved in output/sparse/model/checkpoint-500/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-500/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-500/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-300] due to args.save_total_limit\n"," 15% 600/4000 [13:59<12:40,  4.47it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:16<00:00, 22.76it/s]\n","{'eval_RR@10': 0.6018154761904762, 'eval_nDCG@10': 0.6322970369494461, 'eval_R@1000': 0.9825, 'epoch': 0.4}\n"," 15% 600/4000 [16:15<12:40,  4.47it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-600\n","Configuration saved in output/sparse/model/checkpoint-600/config.json\n","Model weights saved in output/sparse/model/checkpoint-600/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-600/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-600/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-400] due to args.save_total_limit\n"," 18% 700/4000 [16:41<12:33,  4.38it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:16<00:00, 22.70it/s]\n","{'eval_RR@10': 0.6079960317460318, 'eval_nDCG@10': 0.6327232626864947, 'eval_R@1000': 0.9825, 'epoch': 0.46}\n"," 18% 700/4000 [18:58<12:33,  4.38it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-700\n","Configuration saved in output/sparse/model/checkpoint-700/config.json\n","Model weights saved in output/sparse/model/checkpoint-700/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-700/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-700/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-500] due to args.save_total_limit\n"," 20% 800/4000 [19:24<12:46,  4.18it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:16<00:00, 22.79it/s]\n","{'eval_RR@10': 0.6621805555555556, 'eval_nDCG@10': 0.685396433563034, 'eval_R@1000': 0.9825, 'epoch': 0.53}\n"," 20% 800/4000 [21:41<12:46,  4.18it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-800\n","Configuration saved in output/sparse/model/checkpoint-800/config.json\n","Model weights saved in output/sparse/model/checkpoint-800/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-800/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-800/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-600] due to args.save_total_limit\n"," 22% 900/4000 [22:06<11:58,  4.31it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:15<00:00, 22.84it/s]\n","{'eval_RR@10': 0.6697341269841267, 'eval_nDCG@10': 0.6965717578098852, 'eval_R@1000': 0.9825, 'epoch': 0.59}\n"," 22% 900/4000 [24:23<11:58,  4.31it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-900\n","Configuration saved in output/sparse/model/checkpoint-900/config.json\n","Model weights saved in output/sparse/model/checkpoint-900/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-900/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-900/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-700] due to args.save_total_limit\n","{'loss': 0.2943, 'learning_rate': 9.87e-06, 'epoch': 0.66}\n"," 25% 1000/4000 [24:49<12:34,  3.97it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:16<00:00, 22.77it/s]\n","{'eval_RR@10': 0.6857539682539681, 'eval_nDCG@10': 0.7069533996788938, 'eval_R@1000': 0.9825, 'epoch': 0.66}\n"," 25% 1000/4000 [27:05<12:34,  3.97it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-1000\n","Configuration saved in output/sparse/model/checkpoint-1000/config.json\n","Model weights saved in output/sparse/model/checkpoint-1000/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-1000/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-1000/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-800] due to args.save_total_limit\n"," 28% 1100/4000 [27:32<11:46,  4.11it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:17<00:00, 22.65it/s]\n","{'eval_RR@10': 0.7292281746031748, 'eval_nDCG@10': 0.75686679041934, 'eval_R@1000': 0.9825, 'epoch': 0.73}\n"," 28% 1100/4000 [29:49<11:46,  4.11it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-1100\n","Configuration saved in output/sparse/model/checkpoint-1100/config.json\n","Model weights saved in output/sparse/model/checkpoint-1100/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-1100/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-1100/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-900] due to args.save_total_limit\n"," 30% 1200/4000 [30:15<11:08,  4.19it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:17<00:00, 22.61it/s]\n","{'eval_RR@10': 0.757547619047619, 'eval_nDCG@10': 0.7837597029459654, 'eval_R@1000': 0.9825, 'epoch': 0.79}\n"," 30% 1200/4000 [32:32<11:08,  4.19it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-1200\n","Configuration saved in output/sparse/model/checkpoint-1200/config.json\n","Model weights saved in output/sparse/model/checkpoint-1200/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-1200/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-1200/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-1000] due to args.save_total_limit\n"," 32% 1300/4000 [32:58<10:28,  4.29it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:17<00:00, 22.60it/s]\n","{'eval_RR@10': 0.7591031746031746, 'eval_nDCG@10': 0.7819767585509017, 'eval_R@1000': 0.9825, 'epoch': 0.86}\n"," 32% 1300/4000 [35:16<10:28,  4.29it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-1300\n","Configuration saved in output/sparse/model/checkpoint-1300/config.json\n","Model weights saved in output/sparse/model/checkpoint-1300/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-1300/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-1300/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-1100] due to args.save_total_limit\n"," 35% 1400/4000 [35:42<10:26,  4.15it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:16<00:00, 22.69it/s]\n","{'eval_RR@10': 0.6914265873015872, 'eval_nDCG@10': 0.724111933086262, 'eval_R@1000': 0.9825, 'epoch': 0.92}\n"," 35% 1400/4000 [37:59<10:26,  4.15it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-1400\n","Configuration saved in output/sparse/model/checkpoint-1400/config.json\n","Model weights saved in output/sparse/model/checkpoint-1400/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-1400/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-1400/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-1200] due to args.save_total_limit\n","{'loss': 0.1041, 'learning_rate': 1.487e-05, 'epoch': 0.99}\n"," 38% 1500/4000 [38:24<09:14,  4.51it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:16<00:00, 22.74it/s]\n","{'eval_RR@10': 0.7107003968253969, 'eval_nDCG@10': 0.7454288374922265, 'eval_R@1000': 0.9825, 'epoch': 0.99}\n"," 38% 1500/4000 [40:41<09:14,  4.51it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-1500\n","Configuration saved in output/sparse/model/checkpoint-1500/config.json\n","Model weights saved in output/sparse/model/checkpoint-1500/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-1500/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-1500/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-1400] due to args.save_total_limit\n"," 40% 1600/4000 [41:07<09:22,  4.27it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:15<00:00, 22.86it/s]\n","{'eval_RR@10': 0.6791428571428569, 'eval_nDCG@10': 0.7071624891588018, 'eval_R@1000': 0.9825, 'epoch': 1.06}\n"," 40% 1600/4000 [43:23<09:22,  4.27it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-1600\n","Configuration saved in output/sparse/model/checkpoint-1600/config.json\n","Model weights saved in output/sparse/model/checkpoint-1600/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-1600/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-1600/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-1500] due to args.save_total_limit\n"," 42% 1700/4000 [43:48<09:08,  4.20it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:15<00:00, 22.89it/s]\n","{'eval_RR@10': 0.6845476190476191, 'eval_nDCG@10': 0.7184844303367011, 'eval_R@1000': 0.9825, 'epoch': 1.12}\n"," 42% 1700/4000 [46:04<09:08,  4.20it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-1700\n","Configuration saved in output/sparse/model/checkpoint-1700/config.json\n","Model weights saved in output/sparse/model/checkpoint-1700/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-1700/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-1700/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-1600] due to args.save_total_limit\n"," 45% 1800/4000 [46:30<09:09,  4.00it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:16<00:00, 22.81it/s]\n","{'eval_RR@10': 0.6461567460317462, 'eval_nDCG@10': 0.6748625817995679, 'eval_R@1000': 0.9825, 'epoch': 1.19}\n"," 45% 1800/4000 [48:47<09:09,  4.00it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-1800\n","Configuration saved in output/sparse/model/checkpoint-1800/config.json\n","Model weights saved in output/sparse/model/checkpoint-1800/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-1800/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-1800/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-1700] due to args.save_total_limit\n"," 48% 1900/4000 [49:12<07:59,  4.38it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:16<00:00, 22.82it/s]\n","{'eval_RR@10': 0.6632043650793649, 'eval_nDCG@10': 0.6893927928128752, 'eval_R@1000': 0.9825, 'epoch': 1.25}\n"," 48% 1900/4000 [51:29<07:59,  4.38it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-1900\n","Configuration saved in output/sparse/model/checkpoint-1900/config.json\n","Model weights saved in output/sparse/model/checkpoint-1900/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-1900/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-1900/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-1800] due to args.save_total_limit\n","{'loss': 0.0523, 'learning_rate': 1.987e-05, 'epoch': 1.32}\n"," 50% 2000/4000 [51:54<07:55,  4.21it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:15<00:00, 22.87it/s]\n","{'eval_RR@10': 0.6640992063492063, 'eval_nDCG@10': 0.6997544707359117, 'eval_R@1000': 0.9825, 'epoch': 1.32}\n"," 50% 2000/4000 [54:11<07:55,  4.21it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-2000\n","Configuration saved in output/sparse/model/checkpoint-2000/config.json\n","Model weights saved in output/sparse/model/checkpoint-2000/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-2000/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-2000/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-1900] due to args.save_total_limit\n"," 52% 2100/4000 [54:36<07:38,  4.14it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:15<00:00, 22.88it/s]\n","{'eval_RR@10': 0.5683908730158731, 'eval_nDCG@10': 0.604578627641476, 'eval_R@1000': 0.9825, 'epoch': 1.39}\n"," 52% 2100/4000 [56:52<07:38,  4.14it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-2100\n","Configuration saved in output/sparse/model/checkpoint-2100/config.json\n","Model weights saved in output/sparse/model/checkpoint-2100/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-2100/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-2100/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-2000] due to args.save_total_limit\n"," 55% 2200/4000 [57:18<07:23,  4.06it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:15<00:00, 22.92it/s]\n","{'eval_RR@10': 0.5947480158730157, 'eval_nDCG@10': 0.6244167864214517, 'eval_R@1000': 0.9825, 'epoch': 1.45}\n"," 55% 2200/4000 [59:34<07:23,  4.06it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-2200\n","Configuration saved in output/sparse/model/checkpoint-2200/config.json\n","Model weights saved in output/sparse/model/checkpoint-2200/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-2200/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-2200/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-2100] due to args.save_total_limit\n"," 57% 2300/4000 [59:59<06:32,  4.33it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:15<00:00, 22.91it/s]\n","{'eval_RR@10': 0.5449861111111113, 'eval_nDCG@10': 0.5745414985709391, 'eval_R@1000': 0.9825, 'epoch': 1.52}\n"," 57% 2300/4000 [1:02:15<06:32,  4.33it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-2300\n","Configuration saved in output/sparse/model/checkpoint-2300/config.json\n","Model weights saved in output/sparse/model/checkpoint-2300/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-2300/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-2300/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-2200] due to args.save_total_limit\n"," 60% 2400/4000 [1:02:41<05:51,  4.55it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:15<00:00, 22.94it/s]\n","{'eval_RR@10': 0.5012063492063493, 'eval_nDCG@10': 0.5353527733013501, 'eval_R@1000': 0.9825, 'epoch': 1.58}\n"," 60% 2400/4000 [1:04:57<05:51,  4.55it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-2400\n","Configuration saved in output/sparse/model/checkpoint-2400/config.json\n","Model weights saved in output/sparse/model/checkpoint-2400/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-2400/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-2400/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-2300] due to args.save_total_limit\n","{'loss': 0.0502, 'learning_rate': 2.487e-05, 'epoch': 1.65}\n"," 62% 2500/4000 [1:05:23<05:48,  4.31it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:15<00:00, 22.92it/s]\n","{'eval_RR@10': 0.39743253968253983, 'eval_nDCG@10': 0.4316632836981464, 'eval_R@1000': 0.9825, 'epoch': 1.65}\n"," 62% 2500/4000 [1:07:38<05:48,  4.31it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-2500\n","Configuration saved in output/sparse/model/checkpoint-2500/config.json\n","Model weights saved in output/sparse/model/checkpoint-2500/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-2500/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-2500/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-2400] due to args.save_total_limit\n"," 65% 2600/4000 [1:08:04<05:39,  4.12it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:15<00:00, 22.92it/s]\n","{'eval_RR@10': 0.35620634920634925, 'eval_nDCG@10': 0.3868242643136542, 'eval_R@1000': 0.9825, 'epoch': 1.72}\n"," 65% 2600/4000 [1:10:20<05:39,  4.12it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-2600\n","Configuration saved in output/sparse/model/checkpoint-2600/config.json\n","Model weights saved in output/sparse/model/checkpoint-2600/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-2600/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-2600/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-2500] due to args.save_total_limit\n"," 68% 2700/4000 [1:10:46<04:50,  4.48it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:15<00:00, 22.87it/s]\n","{'eval_RR@10': 0.3280535714285715, 'eval_nDCG@10': 0.36594260236777104, 'eval_R@1000': 0.9825, 'epoch': 1.78}\n"," 68% 2700/4000 [1:13:02<04:50,  4.48it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-2700\n","Configuration saved in output/sparse/model/checkpoint-2700/config.json\n","Model weights saved in output/sparse/model/checkpoint-2700/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-2700/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-2700/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-2600] due to args.save_total_limit\n"," 70% 2800/4000 [1:13:28<04:54,  4.08it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:15<00:00, 22.85it/s]\n","{'eval_RR@10': 0.33706547619047617, 'eval_nDCG@10': 0.3712472756948032, 'eval_R@1000': 0.9825, 'epoch': 1.85}\n"," 70% 2800/4000 [1:15:44<04:54,  4.08it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-2800\n","Configuration saved in output/sparse/model/checkpoint-2800/config.json\n","Model weights saved in output/sparse/model/checkpoint-2800/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-2800/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-2800/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-2700] due to args.save_total_limit\n"," 72% 2900/4000 [1:16:09<04:24,  4.16it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:16<00:00, 22.83it/s]\n","{'eval_RR@10': 0.3034603174603175, 'eval_nDCG@10': 0.33618010036657303, 'eval_R@1000': 0.9825, 'epoch': 1.91}\n"," 72% 2900/4000 [1:18:26<04:24,  4.16it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-2900\n","Configuration saved in output/sparse/model/checkpoint-2900/config.json\n","Model weights saved in output/sparse/model/checkpoint-2900/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-2900/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-2900/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-2800] due to args.save_total_limit\n","{'loss': 0.0429, 'learning_rate': 2.9870000000000004e-05, 'epoch': 1.98}\n"," 75% 3000/4000 [1:18:51<04:05,  4.07it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:15<00:00, 22.88it/s]\n","{'eval_RR@10': 0.31543452380952386, 'eval_nDCG@10': 0.33875270132809404, 'eval_R@1000': 0.9825, 'epoch': 1.98}\n"," 75% 3000/4000 [1:21:07<04:05,  4.07it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-3000\n","Configuration saved in output/sparse/model/checkpoint-3000/config.json\n","Model weights saved in output/sparse/model/checkpoint-3000/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-3000/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-3000/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-2900] due to args.save_total_limit\n"," 78% 3100/4000 [1:21:33<03:33,  4.21it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:15<00:00, 22.86it/s]\n","{'eval_RR@10': 0.29715277777777777, 'eval_nDCG@10': 0.3189260352242245, 'eval_R@1000': 0.9825, 'epoch': 2.05}\n"," 78% 3100/4000 [1:23:49<03:33,  4.21it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-3100\n","Configuration saved in output/sparse/model/checkpoint-3100/config.json\n","Model weights saved in output/sparse/model/checkpoint-3100/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-3100/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-3100/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-3000] due to args.save_total_limit\n"," 80% 3200/4000 [1:24:15<03:19,  4.00it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:15<00:00, 22.87it/s]\n","{'eval_RR@10': 0.25148412698412703, 'eval_nDCG@10': 0.2800022644432846, 'eval_R@1000': 0.9825, 'epoch': 2.11}\n"," 80% 3200/4000 [1:26:31<03:19,  4.00it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-3200\n","Configuration saved in output/sparse/model/checkpoint-3200/config.json\n","Model weights saved in output/sparse/model/checkpoint-3200/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-3200/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-3200/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-3100] due to args.save_total_limit\n"," 82% 3300/4000 [1:26:57<02:41,  4.34it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:15<00:00, 22.87it/s]\n","{'eval_RR@10': 0.26861706349206355, 'eval_nDCG@10': 0.2949393978013214, 'eval_R@1000': 0.9825, 'epoch': 2.18}\n"," 82% 3300/4000 [1:29:13<02:41,  4.34it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-3300\n","Configuration saved in output/sparse/model/checkpoint-3300/config.json\n","Model weights saved in output/sparse/model/checkpoint-3300/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-3300/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-3300/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-3200] due to args.save_total_limit\n"," 85% 3400/4000 [1:29:38<02:21,  4.24it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:15<00:00, 22.86it/s]\n","{'eval_RR@10': 0.22747222222222224, 'eval_nDCG@10': 0.2477864677606647, 'eval_R@1000': 0.9825, 'epoch': 2.24}\n"," 85% 3400/4000 [1:31:55<02:21,  4.24it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-3400\n","Configuration saved in output/sparse/model/checkpoint-3400/config.json\n","Model weights saved in output/sparse/model/checkpoint-3400/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-3400/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-3400/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-3300] due to args.save_total_limit\n","{'loss': 0.033, 'learning_rate': 3.487e-05, 'epoch': 2.31}\n"," 88% 3500/4000 [1:32:20<02:01,  4.13it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:15<00:00, 22.86it/s]\n","{'eval_RR@10': 0.19493650793650796, 'eval_nDCG@10': 0.216760218077002, 'eval_R@1000': 0.9825, 'epoch': 2.31}\n"," 88% 3500/4000 [1:34:36<02:01,  4.13it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-3500\n","Configuration saved in output/sparse/model/checkpoint-3500/config.json\n","Model weights saved in output/sparse/model/checkpoint-3500/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-3500/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-3500/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-3400] due to args.save_total_limit\n"," 90% 3600/4000 [1:35:02<01:34,  4.21it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:15<00:00, 22.87it/s]\n","{'eval_RR@10': 0.25699007936507945, 'eval_nDCG@10': 0.2840801489170696, 'eval_R@1000': 0.9825, 'epoch': 2.38}\n"," 90% 3600/4000 [1:37:18<01:34,  4.21it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-3600\n","Configuration saved in output/sparse/model/checkpoint-3600/config.json\n","Model weights saved in output/sparse/model/checkpoint-3600/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-3600/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-3600/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-3500] due to args.save_total_limit\n"," 92% 3700/4000 [1:37:44<01:12,  4.16it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:15<00:00, 22.89it/s]\n","{'eval_RR@10': 0.2659166666666667, 'eval_nDCG@10': 0.2846674107546736, 'eval_R@1000': 0.9825, 'epoch': 2.44}\n"," 92% 3700/4000 [1:40:00<01:12,  4.16it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-3700\n","Configuration saved in output/sparse/model/checkpoint-3700/config.json\n","Model weights saved in output/sparse/model/checkpoint-3700/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-3700/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-3700/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-3600] due to args.save_total_limit\n"," 95% 3800/4000 [1:40:26<00:48,  4.12it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:15<00:00, 22.93it/s]\n","{'eval_RR@10': 0.2539126984126985, 'eval_nDCG@10': 0.272382926522571, 'eval_R@1000': 0.9825, 'epoch': 2.51}\n"," 95% 3800/4000 [1:42:42<00:48,  4.12it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-3800\n","Configuration saved in output/sparse/model/checkpoint-3800/config.json\n","Model weights saved in output/sparse/model/checkpoint-3800/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-3800/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-3800/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-3700] due to args.save_total_limit\n"," 98% 3900/4000 [1:43:09<00:23,  4.27it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:16<00:00, 22.80it/s]\n","{'eval_RR@10': 0.23667063492063495, 'eval_nDCG@10': 0.2642597013483352, 'eval_R@1000': 0.9825, 'epoch': 2.57}\n"," 98% 3900/4000 [1:45:26<00:23,  4.27it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-3900\n","Configuration saved in output/sparse/model/checkpoint-3900/config.json\n","Model weights saved in output/sparse/model/checkpoint-3900/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-3900/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-3900/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-3800] due to args.save_total_limit\n","{'loss': 0.0316, 'learning_rate': 3.987e-05, 'epoch': 2.64}\n","100% 4000/4000 [1:45:52<00:00,  4.29it/s]INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:16<00:00, 22.73it/s]\n","{'eval_RR@10': 0.17991666666666667, 'eval_nDCG@10': 0.1955081426748456, 'eval_R@1000': 0.9825, 'epoch': 2.64}\n","100% 4000/4000 [1:48:09<00:00,  4.29it/s]INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model/checkpoint-4000\n","Configuration saved in output/sparse/model/checkpoint-4000/config.json\n","Model weights saved in output/sparse/model/checkpoint-4000/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/checkpoint-4000/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/checkpoint-4000/special_tokens_map.json\n","Deleting older checkpoint [output/sparse/model/checkpoint-3900] due to args.save_total_limit\n","INFO:neural_ir.trainer.hf_trainer:Running evaluation\n","Evaluating the model: 100% 3106/3106 [02:16<00:00, 22.74it/s]\n","{'eval_RR@10': 0.17991666666666667, 'eval_nDCG@10': 0.1955081426748456, 'eval_R@1000': 0.9825, 'epoch': 2.64}\n","100% 4000/4000 [1:50:27<00:00,  4.29it/s]\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","INFO:neural_ir.trainer.hf_trainer:Loading best model from output/sparse/model/checkpoint-1300 (score: 0.7591031746031746).\n","{'train_runtime': 6628.3175, 'train_samples_per_second': 38.622, 'train_steps_per_second': 0.603, 'train_loss': 5.335811871528626, 'epoch': 2.64}\n","100% 4000/4000 [1:50:28<00:00,  1.66s/it]\n","INFO:neural_ir.trainer.hf_trainer:Saving model checkpoint to output/sparse/model\n","Configuration saved in output/sparse/model/config.json\n","Model weights saved in output/sparse/model/pytorch_model.bin\n","tokenizer config file saved in output/sparse/model/tokenizer_config.json\n","Special tokens file saved in output/sparse/model/special_tokens_map.json\n"]}],"source":["!python -m neural_ir.train --train_batch_size 64 --eval_batch_size 64 sparse"]},{"cell_type":"markdown","metadata":{"id":"XeinjLMiIIzg"},"source":["## 2.2 Prediction "]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1xUWlbSSH6ha","executionInfo":{"status":"ok","timestamp":1678032989189,"user_tz":-60,"elapsed":236225,"user":{"displayName":"Abhinav Bhuyan","userId":"03657681414650155581"}},"outputId":"a1723f60-6932-43bc-8e95-84ffa90941de"},"outputs":[{"output_type":"stream","name":"stdout","text":["2023-03-05 16:12:34.149903: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-03-05 16:12:34.310702: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2023-03-05 16:12:35.106804: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2023-03-05 16:12:35.106917: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2023-03-05 16:12:35.106937: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","reading pairs from collection.tsv: 100% 96270/96270 [00:00<00:00, 692368.15it/s]\n","reading pairs from test_queries.tsv: 100% 500/500 [00:00<00:00, 1150385.08it/s]\n","Encoding documents: 100% 6017/6017 [01:57<00:00, 51.21it/s]\n","Encoding queries and search: 100% 32/32 [00:00<00:00, 72.50it/s]\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\n","2023-03-05 16:14:41,209 INFO  [main] index.IndexCollection (IndexCollection.java:391) - Setting log level to INFO\n","2023-03-05 16:14:41,211 INFO  [main] index.IndexCollection (IndexCollection.java:394) - Starting indexer...\n","2023-03-05 16:14:41,212 INFO  [main] index.IndexCollection (IndexCollection.java:395) - ============ Loading Parameters ============\n","2023-03-05 16:14:41,212 INFO  [main] index.IndexCollection (IndexCollection.java:396) - DocumentCollection path: output/sparse/docs\n","2023-03-05 16:14:41,213 INFO  [main] index.IndexCollection (IndexCollection.java:397) - CollectionClass: JsonVectorCollection\n","2023-03-05 16:14:41,213 INFO  [main] index.IndexCollection (IndexCollection.java:398) - Generator: DefaultLuceneDocumentGenerator\n","2023-03-05 16:14:41,213 INFO  [main] index.IndexCollection (IndexCollection.java:399) - Threads: 12\n","2023-03-05 16:14:41,214 INFO  [main] index.IndexCollection (IndexCollection.java:400) - Language: en\n","2023-03-05 16:14:41,214 INFO  [main] index.IndexCollection (IndexCollection.java:401) - Stemmer: porter\n","2023-03-05 16:14:41,215 INFO  [main] index.IndexCollection (IndexCollection.java:402) - Keep stopwords? false\n","2023-03-05 16:14:41,215 INFO  [main] index.IndexCollection (IndexCollection.java:403) - Stopwords: null\n","2023-03-05 16:14:41,216 INFO  [main] index.IndexCollection (IndexCollection.java:404) - Store positions? false\n","2023-03-05 16:14:41,216 INFO  [main] index.IndexCollection (IndexCollection.java:405) - Store docvectors? false\n","2023-03-05 16:14:41,216 INFO  [main] index.IndexCollection (IndexCollection.java:406) - Store document \"contents\" field? false\n","2023-03-05 16:14:41,217 INFO  [main] index.IndexCollection (IndexCollection.java:407) - Store document \"raw\" field? false\n","2023-03-05 16:14:41,217 INFO  [main] index.IndexCollection (IndexCollection.java:408) - Additional fields to index: []\n","2023-03-05 16:14:41,218 INFO  [main] index.IndexCollection (IndexCollection.java:409) - Optimize (merge segments)? false\n","2023-03-05 16:14:41,218 INFO  [main] index.IndexCollection (IndexCollection.java:410) - Whitelist: null\n","2023-03-05 16:14:41,218 INFO  [main] index.IndexCollection (IndexCollection.java:411) - Pretokenized?: true\n","2023-03-05 16:14:41,219 INFO  [main] index.IndexCollection (IndexCollection.java:412) - Index path: output/sparse/index\n","2023-03-05 16:14:41,222 INFO  [main] index.IndexCollection (IndexCollection.java:450) - ============ Indexing Collection ============\n","2023-03-05 16:14:41,589 INFO  [main] index.IndexCollection (IndexCollection.java:565) - Thread pool with 12 threads initialized.\n","2023-03-05 16:14:41,589 INFO  [main] index.IndexCollection (IndexCollection.java:567) - Initializing collection in output/sparse/docs\n","2023-03-05 16:14:41,625 INFO  [main] index.IndexCollection (IndexCollection.java:576) - 1 file found\n","2023-03-05 16:14:41,625 INFO  [main] index.IndexCollection (IndexCollection.java:577) - Starting to index...\n","2023-03-05 16:15:41,630 INFO  [main] index.IndexCollection (IndexCollection.java:589) - 60,000 documents indexed\n","2023-03-05 16:16:07,126 DEBUG [pool-2-thread-1] index.IndexCollection$LocalIndexerThread (IndexCollection.java:356) - docs/docs.jsonl: 96270 docs added.\n","2023-03-05 16:16:08,659 INFO  [main] index.IndexCollection (IndexCollection.java:633) - Indexing Complete! 96,270 documents indexed\n","2023-03-05 16:16:08,659 INFO  [main] index.IndexCollection (IndexCollection.java:634) - ============ Final Counter Values ============\n","2023-03-05 16:16:08,659 INFO  [main] index.IndexCollection (IndexCollection.java:635) - indexed:           96,270\n","2023-03-05 16:16:08,659 INFO  [main] index.IndexCollection (IndexCollection.java:636) - unindexable:            0\n","2023-03-05 16:16:08,659 INFO  [main] index.IndexCollection (IndexCollection.java:637) - empty:                  0\n","2023-03-05 16:16:08,660 INFO  [main] index.IndexCollection (IndexCollection.java:638) - skipped:                0\n","2023-03-05 16:16:08,660 INFO  [main] index.IndexCollection (IndexCollection.java:639) - errors:                 0\n","2023-03-05 16:16:08,666 INFO  [main] index.IndexCollection (IndexCollection.java:642) - Total 96,270 documents indexed in 00:01:27\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","2023-03-05 16:16:12.795350: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2023-03-05 16:16:12.795460: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2023-03-05 16:16:12.795484: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","Running output/sparse/queries/test.tsv topics, saving to output/sparse/test_run.trec...\n","100% 500/500 [00:09<00:00, 52.17it/s]\n"]}],"source":["!python -m neural_ir.rank_sparse"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}